{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "### Main Project 1 script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZUIDFL5jxvxe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "D7FSvO5_xvxj"
   },
   "outputs": [],
   "source": [
    "# categories = ['comp.graphics', 'comp.os.ms-windows.misc',\n",
    "#               'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
    "#               'rec.autos', 'rec.motorcycles',\n",
    "#               'rec.sport.baseball', 'rec.sport.hockey']\n",
    "# train_dataset = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "# test_dataset = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ob_91Lq_xvxl"
   },
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "0WEfap6yxvxm",
    "outputId": "dc8a8680-a3b7-420a-bb63-cb2aaf14810e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116ffefd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fetch all 20 news groups categories and plot a histogram of the training documents.\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
    "plt.hist(newsgroups_train.target, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aIGvgdKMxvxr"
   },
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PCUxRNaDxvxs"
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Fetching 20NewsGroups dataset\n",
    "\n",
    "categories = ['comp.graphics', 'comp.os.ms-windows.misc',\n",
    "              'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
    "              'rec.autos', 'rec.motorcycles',\n",
    "              'rec.sport.baseball', 'rec.sport.hockey']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', # choose which subset of the dataset to use; can be 'train', 'test', 'all'\n",
    "                                  categories=categories, # choose the categories to load; if is `None`, load all categories\n",
    "                                  shuffle=True,\n",
    "                                  random_state=42, # set the seed of random number generator when shuffling to make the outcome repeatable across different runs\n",
    "                                  # remove=['headers'],\n",
    "                                  )\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "Y1Aq6iCbxvxu"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2522e8486c55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# print(lemmatized_data[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mlemmatized_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatize_and_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwenty_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0mlemmatized_testing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatize_and_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwenty_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-2522e8486c55>\u001b[0m in \u001b[0;36mlemmatize_and_filter\u001b[0;34m(documents)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# lemmatize the document:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mtraining_tagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mlemmatized_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatize_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# remove numbers from document:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-2522e8486c55>\u001b[0m in \u001b[0;36mlemmatize_training\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Text input is string, returns array of lowercased strings(words).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n\u001b[0;32m---> 30\u001b[0;31m             for word, tag in pos_tag(nltk.word_tokenize(text))]\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dmwang626/anaconda/lib/python3.5/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \"\"\"\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     return [\n\u001b[1;32m    145\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dmwang626/anaconda/lib/python3.5/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[1;32m    104\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dmwang626/anaconda/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \"\"\"\n\u001b[0;32m-> 1269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dmwang626/anaconda/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \"\"\"\n\u001b[0;32m-> 1323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dmwang626/anaconda/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \"\"\"\n\u001b[0;32m-> 1323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dmwang626/anaconda/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1313\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1314\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dmwang626/anaconda/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \"\"\"\n\u001b[1;32m   1353\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dmwang626/anaconda/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dmwang626/anaconda/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "# Perform lemmatization on dataset\n",
    "\n",
    "# The lemmatizer is actually pretty complicated, it needs Parts of Speech (POS) tags\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "# nltk.download('punkt')#, if you need \"tokenizers/punkt/english.pickle\", choose it\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN': 'n', 'JJ': 'a',\n",
    "                  'VB': 'v', 'RB': 'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n'\n",
    "\n",
    "\n",
    "# def lemmatize_sent(list_word, wnl):\n",
    "#     # Text input is string, returns array of lowercased strings(words).\n",
    "#     return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n",
    "#             for word, tag in pos_tag(list_word)]\n",
    "\n",
    "\n",
    "wnl = nltk.wordnet.WordNetLemmatizer()\n",
    "def lemmatize_training(text):\n",
    "    # Text input is string, returns array of lowercased strings(words).\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n",
    "            for word, tag in pos_tag(nltk.word_tokenize(text))]\n",
    "\n",
    "\n",
    "# TODO: should this filter out the following numbers too? \"4-5\" \"c650\"\n",
    "def filter_numbers(text_array):\n",
    "    # Filter out any numbers found in the array of strings\n",
    "    output = []\n",
    "    for s in text_array:\n",
    "        if not s.isdigit():\n",
    "            # if not a digit...\n",
    "            try:\n",
    "                # if a float, filter out\n",
    "                float(s)\n",
    "            except ValueError:\n",
    "                # if not a float, add to output\n",
    "                output.append(s)\n",
    "        else:\n",
    "            # if a digit, filter out\n",
    "            pass\n",
    "    return output\n",
    "\n",
    "\n",
    "def array_to_string(text_array, delimeter=\"\"):\n",
    "    # Converts an array back into a string of words using the provided delimeter to add between each word\n",
    "    output = \"\"\n",
    "    for s in text_array:\n",
    "        output = output + delimeter + s\n",
    "    return output\n",
    "\n",
    "\n",
    "def lemmatize_and_filter(documents):\n",
    "    # Performs lemmatization, and number filtering on the given documents\n",
    "    lemmatized_data = []\n",
    "    for i in documents:\n",
    "        # lemmatize the document:\n",
    "        training_tagged = pos_tag(nltk.word_tokenize(i))\n",
    "        lemmatized_array = lemmatize_training(i)\n",
    "\n",
    "        # remove numbers from document:\n",
    "        filtered_array = filter_numbers(lemmatized_array)\n",
    "\n",
    "        # reassemble back to string:\n",
    "        lemmatized_string = array_to_string(filtered_array, ' ')\n",
    "\n",
    "        # add to final data list\n",
    "        # print(lemmatized_string)\n",
    "        lemmatized_data.append(lemmatized_string)\n",
    "\n",
    "    return lemmatized_data\n",
    "\n",
    "\n",
    "# print(lemmatized_data[0])\n",
    "lemmatized_training = lemmatize_and_filter(twenty_train.data)\n",
    "lemmatized_testing = lemmatize_and_filter(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "BjA770b7xvxw"
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Push lemmatized documents through CountVectorizer\n",
    "\n",
    "# count_vect = CountVectorizer(min_df=3)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# do for training\n",
    "count_vect = CountVectorizer(min_df=3, stop_words='english')\n",
    "X_lemmatized_train_counts = count_vect.fit_transform(lemmatized_training)\n",
    "\n",
    "# do for testing\n",
    "X_lemmatized_test_counts = count_vect.transform(lemmatized_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "Z0Ju4BQ8xvxz",
    "outputId": "11b41fa6-8266-401a-90b7-f27aaf7151c6"
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Report shapes of TF-IDF matrices\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# do for training\n",
    "X_lemmatized_train_tfidf = tfidf_transformer.fit_transform(X_lemmatized_train_counts)\n",
    "\n",
    "print(X_lemmatized_train_tfidf.shape)\n",
    "print('-' * 20)\n",
    "print(X_lemmatized_train_counts.toarray()[:30, :5])\n",
    "print('-' * 20)\n",
    "print(X_lemmatized_train_tfidf.toarray()[:30, :5])\n",
    "\n",
    "# do for testing\n",
    "X_lemmatized_test_tfidf = tfidf_transformer.transform(X_lemmatized_test_counts)\n",
    "\n",
    "print(X_lemmatized_test_tfidf.shape)\n",
    "print('-' * 20)\n",
    "print(X_lemmatized_test_counts.toarray()[:30, :5])\n",
    "print('-' * 20)\n",
    "print(X_lemmatized_test_tfidf.toarray()[:30, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "5o5ftdgBxvx3"
   },
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "MVuiiQJoxvx3",
    "outputId": "a5da2dcb-8bc2-45d9-d9b2-aa949a9c6ff9"
   },
   "outputs": [],
   "source": [
    "# Perform LSI using the truncated SVD\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "X_lsi_train_reduced = svd.fit_transform(X_lemmatized_train_tfidf)\n",
    "Y_lsi_train_reduced = svd.components_\n",
    "print(X_lsi_train_reduced.shape)\n",
    "print(svd.components_.shape)\n",
    "\n",
    "X_lsi_test_reduced = svd.transform(X_lemmatized_test_tfidf)\n",
    "Y_lsi_test_reduced = svd.components_\n",
    "print(X_lsi_train_reduced.shape)\n",
    "print(svd.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "URSfInKOxvx6",
    "outputId": "d93fc1e9-0aac-4211-e920-12872703dde2"
   },
   "outputs": [],
   "source": [
    "# Perform NMF\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "model = NMF(n_components=50, init='random', random_state=42)\n",
    "W_nmf_train_reduced = model.fit_transform(X_lemmatized_train_tfidf)\n",
    "H_nmf_train_reduced = model.components_\n",
    "\n",
    "print(W_nmf_train_reduced.shape)\n",
    "print(H_nmf_train_reduced.shape)\n",
    "\n",
    "W_nmf_test_reduced = model.transform(X_lemmatized_test_tfidf)\n",
    "H_nmf_test_reduced = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "AfGbsKGhxvx-",
    "outputId": "9ebacd84-439f-4a77-e1dc-694ab155539b"
   },
   "outputs": [],
   "source": [
    "# Compare LSI and NMF\n",
    "\n",
    "nmf_val = np.linalg.norm(X_lemmatized_train_tfidf - np.matmul(W_nmf_train_reduced, H_nmf_train_reduced), 'fro')\n",
    "lsi_val = np.linalg.norm(X_lemmatized_train_tfidf - np.matmul(X_lsi_train_reduced, Y_lsi_train_reduced), 'fro')\n",
    "\n",
    "print('NMF: ', nmf_val)\n",
    "print('LSI: ', lsi_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train an unregularized logistic regression classifier.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# To be unregularized, we make the inverse of the regularization strength C \n",
    "# to be large to approximate an unregularized classifier.\n",
    "clf = LogisticRegression(random_state=42, C=500, solver='lbfgs').fit(X_lsi_train_reduced, twenty_train.target)\n",
    "\n",
    "# score = clf.decision_function(X_lsi_test_reduced)\n",
    "predicted = clf.predict(X_lsi_test_reduced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find confusion matrix, accuracy, precision-recall, and F-1 scores\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('Confusion matrix: \\n', confusion_matrix(twenty_test.target, predicted))\n",
    "\n",
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: ', accuracy_score(twenty_test.target, predicted))\n",
    "\n",
    "# Average precision-recall score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print('Average precision-recall score:', average_precision_score(twenty_test.target, predicted))\n",
    "print('Precision score: ', precision_score(twenty_test.target, predicted))\n",
    "print('Recall score: ', recall_score(twenty_test.target, predicted))\n",
    "\n",
    "# F-1 score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('F-1 score:', f1_score(twenty_test.target, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "score = clf.decision_function(X_lsi_test_reduced)\n",
    "fpr, tpr, thresholds = roc_curve(twenty_test.target, score)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title('ROC curve for unregularized logistic regression')\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.xlim(left=-0.02)\n",
    "plt.ylim(top=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UGNn6K_exvyB"
   },
   "source": [
    "#### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5NkfS2HZxvyB"
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Train a Naive Bayes Gaussian classifier on the reduced TFIDF training set from problem 3\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = GaussianNB().fit(W_nmf_train_reduced, twenty_train.target)\n",
    "clf2 = MultinomialNB().fit(W_nmf_train_reduced, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "kQN9jIzvxvyE",
    "outputId": "6308ec42-fcf7-4348-a564-562c886cda29"
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Generate predictions for test set\n",
    "\n",
    "predicted = clf.predict(W_nmf_test_reduced)\n",
    "correct = 0\n",
    "for i, category in enumerate(predicted):\n",
    "    if category == twenty_test.target[i]:\n",
    "        correct += 1\n",
    "    if i < 5:\n",
    "        print('{} =? {}'.format(twenty_test.target_names[category], twenty_test.target_names[twenty_test.target[i]]))\n",
    "    elif i == 5:\n",
    "        print('...\\n')\n",
    "print('Accuracy of NB Gaussian: {}'.format(correct / W_nmf_test_reduced.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "h0IChMnAxvyH"
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Generate predictions for test set\n",
    "\n",
    "predicted = clf2.predict(W_nmf_test_reduced)\n",
    "correct = 0\n",
    "for i, category in enumerate(predicted):\n",
    "    if category == twenty_test.target[i]:\n",
    "        correct += 1\n",
    "    if i < 5:\n",
    "        print('{} =? {}'.format(twenty_test.target_names[category], twenty_test.target_names[twenty_test.target[i]]))\n",
    "    elif i == 5:\n",
    "        print('...\\n')\n",
    "print('Accuracy of NB Gaussian: {}'.format(correct / W_nmf_test_reduced.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wp2QQX5ByD0-"
   },
   "source": [
    "#### TEST"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "proj1.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
